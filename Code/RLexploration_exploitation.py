from scipy.optimize import curve_fit
import os, time, random
import numpy as np
import matplotlib.pyplot as plt
import pickle
from matplotlib.colors import ListedColormap


#build the maze environment from the first paper. define the reward, state, action, etc.
class Qmaze(object):#Environment
    def __init__(self, maze, rat=(15,0)):#start location
        self._maze = np.array(maze)
        nrows, ncols = self._maze.shape
        self.target = (17, 21)   # target cell where the "water" is
        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]#walk through and check '1' in maze matrix
        self.free_cells.remove(self.target)
        if self._maze[self.target] == 0.0:
            raise Exception("Invalid maze: target cell cannot be blocked!")
        if not rat in self.free_cells:
            raise Exception("Invalid Rat Location: must sit on a free cell")
        self.reset(rat)

    def reset(self, rat):#reset the state,reward and visited
        self.rat = rat
        self.maze = np.copy(self._maze)
        nrows, ncols = self.maze.shape
        row, col = rat
        self.maze[row, col] = rat_mark
        self.state = (row, col, 'valid')#valid means road,
        self.total_reward = 0
        self.visited = []#store all of the location where rat got
        return self.state

    def update_state(self, action):#update the state after the action
        nrows, ncols = self.maze.shape
        nrow, ncol, nmode = rat_row, rat_col, mode = self.state

        if self.maze[rat_row, rat_col] > 0.0:
            self.visited.append((rat_row, rat_col))  # record visited cell

        valid_actions = self.valid_actions()
        if not valid_actions:
            nmode = 'blocked'
        elif action in valid_actions:
            nmode = 'valid'
            if action == LEFT:
                ncol -= 1
            elif action == UP:
                nrow -= 1
            if action == RIGHT:
                ncol += 1
            elif action == DOWN:
                nrow += 1
        else:                  # invalid action, no change in rat position
            mode = 'invalid'

        # new state
        self.state = (nrow, ncol, nmode)

    def get_reward(self):#get reward after action, can change the value of reward
        rat_row, rat_col, mode = self.state
        nrows, ncols = self.maze.shape
        if rat_row == 17 and rat_col == 21:# water port position
            return 2
        if (rat_row, rat_col) in self.visited:#reduce repeat visits
            return -0.1
        if mode == 'invalid':# wall
            return -0.75
        if mode == 'valid':#
            return -0.01


    def act(self, action):#apply the action, update state, get reward.
        self.record_reward=[]
        self.update_state(action)
        reward = self.get_reward()
        self.record_reward.append(reward)
        status = self.game_status()
        envstate = self.observe()
        return (envstate, reward, self.state, status)

    def observe(self):#observe the location of the rat
        canvas = self.draw_env()
        envstate = canvas.reshape((1, -1))
        return envstate

    def draw_env(self):#draw the maze
        canvas = np.copy(self.maze)
        nrows, ncols = self.maze.shape
        # clear all visual marks
        for r in range(nrows):
            for c in range(ncols):
                if canvas[r,c] > 0.0:
                    canvas[r,c] = 1.0
        # draw the rat position
        row, col, valid = self.state
        canvas[row, col] = rat_mark
        return canvas

    def game_status(self):
        rat_row, rat_col, mode = self.state
        nrows, ncols = self.maze.shape
        if rat_row == 17 and rat_col == 21:
            self.visited.append((17,21))
            return True
        else:
            return False


    def valid_actions(self, cell=None, prev_action=None):
        if cell is None:
            row, col, mode = self.state
        else:
            row, col = cell
        actions = [0, 1, 2, 3]
        nrows, ncols = self.maze.shape


        if row == 0:
            actions.remove(1)
        elif row == nrows - 1:
            actions.remove(3)
        if col == 0:
            actions.remove(0)
        elif col == ncols - 1:
            actions.remove(2)

        if row > 0 and self.maze[row - 1, col] == 0.0:
            actions.remove(1)  
        if row < nrows - 1 and self.maze[row + 1, col] == 0.0:
            actions.remove(3)  
        if col > 0 and self.maze[row, col - 1] == 0.0:
            actions.remove(0) 
        if col < ncols - 1 and self.maze[row, col + 1] == 0.0:
            actions.remove(2)

        if prev_action is not None:
            prev_row = row - (prev_action == 3) + (prev_action == 1)
            prev_col = col + (prev_action == 0) - (prev_action == 2)
            if prev_row == row:
                if prev_col < col and self.maze[row, col+1] ==1.0 and self.maze[row+1, col] == 0 and self.maze[row-1, col] == 0:
                    actions.remove(0)
                elif prev_col > col and self.maze[row, col-1] ==1.0 and self.maze[row+1, col] == 0 and self.maze[row-1, col] == 0:
                    actions.remove(2)
            elif prev_col == col:
                if prev_row < row and self.maze[row + 1, col] == 1.0 and self.maze[row, col+1] == 0 and self.maze[row, col-1] == 0:
                    actions.remove(1)  
                elif prev_row > row and self.maze[row - 1, col] ==1.0 and self.maze[row, col+1] == 0 and self.maze[row, col-1] == 0:

                    actions.remove(3) 

        return actions


class QLearningAgent:
    def __init__(self, env, learning_rate=0.1, discount_factor=0.97, temperature=0.1, epsilon= 0.1, qtable_path=False):
        self.env = env
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.temperature = temperature
        self.epsilon = epsilon
        if qtable_path == False:
            self.q_table = {}
        else:
            with open(qtable_path, 'rb') as f:
                loaded_Q_table = pickle.load(f)
            print("Loaded Q table:", loaded_Q_table)
            new_q_table = {}
            for state_str, actions in loaded_Q_table.items():

                new_q_table[state_str] = actions
            self.q_table = new_q_table

    def choose_action_egreedy(self, state,  pre_action):#epsilon-greedy
        if np.random.uniform(0, 1) < self.epsilon:
            return np.random.choice(self.env.valid_actions(prev_action = pre_action))
        else:
            if state not in self.q_table:
                return np.random.choice(self.env.valid_actions(prev_action = pre_action))
            else:
                return max(self.q_table[state], key=self.q_table[state].get)

    def choose_action_softmax(self, state,pre_action):#softmax
            state_str = str(state)
            if state_str not in self.q_table:
                return np.random.choice(self.env.valid_actions(prev_action =pre_action))
            q_values = np.array([self.q_table[state_str].get(action, 0) for action in self.env.valid_actions(prev_action =pre_action)])
            softmax_probs = np.exp((q_values - np.max(q_values)) / self.temperature) / np.sum(np.exp((q_values - np.max(q_values)) / self.temperature))
            action = np.random.choice(self.env.valid_actions(prev_action = pre_action), p=softmax_probs)
            return action


    def update_q_table(self, state, action, reward, next_state):
        state_str = str(state)
        next_state_str = str(next_state)

        if state_str not in self.q_table:
            self.q_table[state_str] = {action: reward}
        else:
            if action not in self.q_table[state_str]:
                self.q_table[state_str][action] = reward
            else:
                next_max = 0
                if next_state_str in self.q_table:
                    # Consider the Q-value of the action chosen by the current policy in the next state
                    next_max = max(self.q_table[next_state_str].values())

                new_value = self.q_table[state_str][action] + self.learning_rate * (reward + self.discount_factor * next_max - self.q_table[state_str][action])
                self.q_table[state_str][action] = new_value





    def train(self, num_episodes):
        rewards = []
        count_record = [0]
        control_record = [0]
        average_reward = 0
        num_reward=0
        water_runs = [0]
        count_reward = [0]

        for episode in range(num_episodes):
            if episode > 7000:
              start_points = [(1,1),(5,1),(9,1),(25,1),(27,1),(1,15),(5,15),(9,15),(17,15)]
              start = random.choice(start_points)
              state = self.env.reset(start)
            else:
              state = self.env.reset((15,0))#（1,7）

            done = False
            total_reward = 0
            count = 0
            control = 0
            water_run = 0
            pre_action = None
            while not done:
                action = self.choose_action_softmax(state,pre_action)
                pre_action = action
                #print(action)
                (canvas, reward, next_state, done) = self.env.act(action)
                self.update_q_table(state, action, reward, next_state)
                total_reward += reward
                state = next_state
                count += 1
                if state in ((13, 21, 'valid'), (13, 1, 'valid'), (17, 1, 'valid')):
                  control += 1
            rewards.append(total_reward)
            
            for i in range(len(self.env.visited)):
              if self.env.visited[i] not in reward_path:
                  num_reward += 1
                  break
              if i == len(self.env.visited)-1:
                water_run += 1
            count_reward.append(num_reward)
            water_runs.append(water_runs[-1]+water_run)
            count_record.append(count_record[-1]+count)
            control_record.append(control_record[-1]+(control/3))
            #softmax_reward4.append(total_reward)
            #visited4.append(len(self.env.visited))
            print(f"Episode {episode+1}, Total Reward: {total_reward}, Down:{done}, Count:{count}")
            average_reward += total_reward
            if (episode+1) % 10 == 0:
              show(env,episode,total_reward,rewards,count_record,control_record,water_runs,count_reward)
              print("Average: ",average_reward/10)
              average_reward = 0






        print('qtable: ',self.q_table)
        #with open(r'/content/drive/MyDrive/Rosenberg-2021-Repository-main/q_table_softmax_200.pkl', 'wb') as f:
         #   pickle.dump(self.q_table, f)



def show(qmaze, episodes, reward, rewards, count_record, control_record, water_run,count_reward):

    trajectory_map = np.zeros_like(qmaze.maze)
    cmap = plt.cm.jet  
    trajectory_cmap = ListedColormap(cmap(np.linspace(0, 1, len(qmaze.visited))))
    fig, ax = plt.subplots()
    row_prev, col_prev = qmaze.visited[0]


    for i in range(1, len(qmaze.visited)):
        row_curr, col_curr = qmaze.visited[i]
        trajectory_map[row_curr, col_curr] = i
        ax.plot([col_prev, col_curr], [row_prev, row_curr], color=trajectory_cmap(i), linewidth=2)
        row_prev, col_prev = row_curr, col_curr

    ax.imshow(qmaze.maze, cmap='gray')
    plt.xticks([])
    plt.yticks([])


    cbar = fig.colorbar(plt.cm.ScalarMappable(cmap=trajectory_cmap),ax=ax)
    cbar.set_label('Time')
    #if (episodes+1) % 1 == 0:
       # plt.savefig('/content/drive/MyDrive/Rosenberg-2021-Repository-main/outcome/maze_trajectory_'+str(episodes+1)+'.png')
    plt.show()
    r = plt.figure()
    plt.plot(count_record, control_record, label='Control Record',color='blue')
    plt.plot(count_record, count_reward, label='Count Reward',color='green')
    plt.plot(count_record[0:len(water_run)], water_run,label='Water run',color='red')
    plt.legend()
    plt.title('Rewards VS Steps')
    plt.xlabel('Steps')
    plt.ylabel('Rewards')
    total = plt.figure()
    best_reward = 1.66
    episode = range(1, len(rewards) + 1)
    plt.plot(episode, rewards, color='black', linestyle='-', label='Rewards')
    average_reward = np.mean(rewards)
    plt.axhline(y=best_reward, color='green', linestyle='-.', linewidth=2, label='Best Reward')
    plt.axhline(y=average_reward, color='red', linestyle='-.', linewidth=2, label='Average Reward')
    plt.title('Episode vs Total Reward')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    plt.grid(True)
    #if (episodes+1) % 100 == 0:
      #r.savefig('/content/drive/MyDrive/Rosenberg-2021-Repository-main/outcome/rewards_'+str(episodes+1)+'.png')
      #total.savefig('/content/drive/MyDrive/Rosenberg-2021-Repository-main/outcome/global_'+str(episodes+1)+'.png')
    plt.tight_layout()
    plt.show()

def plot_mazenode_with_labels(maze, labels):
    plt.figure(figsize=(10, 10))
    plt.imshow(maze, cmap='gray')

    num_rows, num_cols = maze.shape

    for row in range(num_rows):
        for col in range(num_cols):
            if maze[row, col] != 0:  
                plt.text(col, row, str(int(labels[row, col])-1), color='blue', ha='center', va='center')

    plt.axis('off')
    plt.show()

def logarithmic_func(x, a, b):
    return a + b * np.log(x)


def fit_curve(visited1, visited2, visited3,visited4):
    all_arrays = np.array([visited1, visited2, visited3, visited4])
    x_data = np.array(range(1, 101)) 
    y_data = np.median(all_arrays, axis=0)
    popt, pcov = curve_fit(logarithmic_func, x_data, y_data)


    print("Parameter:", popt)

    plt.scatter(x_data,visited1,s=1.5,color='lightgreen',label='All_4')
    plt.scatter(x_data,visited2,s=1.5,color='lightgreen')
    plt.scatter(x_data,visited3,s=1.5,color='lightgreen')
    plt.scatter(x_data,visited4,s=1.5,color='lightgreen')
    plt.scatter(x_data, y_data,s=1.5, color='darkgreen',label='median')
    plt.plot(x_data, logarithmic_func(x_data, *popt), color = 'darkgreen',linewidth=1.5, label='Fitted curve') 
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.ylim(top=2,bottom=-300)
    plt.legend()
    plt.tight_layout()
    plt.show()


visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8
rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5
LEFT = 0
UP = 1
RIGHT = 2
DOWN = 3

# Actions dictionary
actions_dict = {
    LEFT: 'left',
    UP: 'up',
    RIGHT: 'right',
    DOWN: 'down',
}
#maze matrix
Maze = np.array([
[0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.],
[0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.],
[0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.],
[0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.],
[0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.],
[0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.],
[0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.],
[0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.],
[0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.],
[0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.,1.,0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.],
[0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.],
[0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.],
[0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.],
[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.],
[0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.],
[0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.],
[0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.,1.,0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.],
[0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.],
[0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.],
[0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,0.],
[0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.],
[0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,1.,0.,1.,1.,1.,0.],
[0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,1.,0.,0.,1.,0.,0.],
[0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,0.,0.],
[0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.,0.,0.,0.,1.,0.,0.],
[0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.,0.,0.,1.,1.,1.,0.],
[0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.]])

#node of reward path 
reward_path = [(1,7),(1,8),
         (2,8),(3,8),
         (3,7),(3,6),(3,5),
         (4,5),(5,5),(6,5),(7,5),
         (7,6),(7,7),(7,8),(7,9),(7,10),(7,11),
         (8,11),(9,11),(10,11),(11,11),(12,11),(13,11),(14,11),(15,11),(16,11),(17,11),(18,11),(19,11),(20,11),(21,11),(22,11),(23,11),
         (23,12),(23,13),(23,14),(23,15),(23,16),(23,17),
         (22,17),(21,17),(20,17),(19,17),
         (19,18),(19,19),(19,20),
         (18,20),(17,20),
         (17,21)]
#maze node number matrix
num_node=np.array([
[0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0., 0., 0., 0., 0., 0.],
[0.,  197.,  134.,  198.,  0.,  0.,  0.,  201.,  138.,  202.,  0.,  0.,  0.,  213.,  150.,  214.,  0., 0., 0., 217., 154., 218., 0.],
[0.,  0.,  133.,  0.,  0.,  0.,  0.,  0.,  137.,  0.,  0.,  0.,  0.,  0.,  149.,  0.,  0., 0., 0., 0., 153., 0., 0.],
[0.,  0.,  87.,  86.,  85.,  56.,  88.,  89., 90.,  0.,  0.,  0.,  0.,  0.,99.,98.,  97., 64., 100., 101.,  102., 0., 0.],
[0.,  0.,  135.,  0.,  0.,  55.,  0.,  0.,  139.,  0.,  0.,  0.,  0.,  0.,  151.,  0.,  0., 63., 0., 0., 155., 0., 0.],
[0.,  199.,  136.,  200.,  0.,  54.,  0.,  203.,  140.,  204.,  0.,  0.,  0.,  215.,  152.,  216.,  0., 62., 0., 219., 156., 220., 0.],
[0.,  0.,  0.,  0.,  0.,  53.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 61., 0., 0., 0., 0., 0.],
[0.,  0.,  0.,  0.,  0.,  34.,  33.,  32.,  31.,  30.,  29.,  20.,  35., 36.,  37.,38.,39.,40., 0., 0., 0., 0., 0.],
[0.,  0.,  0.,  0.,  0.,  57.,  0.,  0.,  0.,  0.,  0.,  19.,  0.,  0.,  0.,  0.,  0., 65., 0., 0., 0., 0., 0.],
[0.,  205.,  142.,  206.,  0.,  58.,  0.,  209.,  146.,  210.,  0.,  18.,  0.,  221.,  158.,  222.,  0., 66., 0., 225., 162., 226., 0.],
[0.,  0.,  141.,  0.,  0.,  59.,  0.,  0.,  145.,  0.,  0.,  17.,  0.,  0.,  157.,  0.,  0., 67., 0., 0., 161., 0., 0.],
[0.,  0.,  93.,  92.,  91.,  60.,  94., 95.,  96.,  0.,  0.,  16.,  0.,  0.,  105.,  104.,  103., 68., 106., 107., 108., 0., 0.],
[0.,  0.,  143.,  0.,  0.,  0.,  0.,  0.,  147.,  0.,  0.,  15.,  0.,  0.,  159.,  0.,  0., 0., 0., 0.,  163., 0., 0.],
[0.,  207.,  144.,  208.,  0.,  0.,  0.,  211.,  148.,  212.,  0.,  14.,  0.,  223.,  160.,  224.,  0., 0., 0., 227.,  164., 228., 0.],
[0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  13.,  0.,  0.,  0.,  0.,  0., 0., 0., 0., 0., 0., 0.],
[1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  10., 11.,  12.,  0.,  0.,  0.,  0.,  0., 0., 0., 0., 0., 0., 0.],
[0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  21.,  0.,  0.,  0.,  0.,  0., 0., 0., 0., 0., 0., 0.],
[0.,  229.,  166.,  230.,  0.,  0.,  0.,  233.,  170.,  234.,  0.,  22.,  0.,  245.,  182.,  246.,  0., 0., 0., 249., 186., 250., 0.],
[0.,  0.,  165.,  0.,  0.,  0.,  0.,  0.,  169.,  0.,  0.,  23.,  0.,  0.,  181.,  0.,  0., 0., 0., 0., 185., 0., 0.],
[0.,  0.,  111.,  110., 109.,72., 112.,113.,  114.,  0.,  0.,  24.,  0.,  0.,  123.,  122.,  121., 80., 124., 125., 126., 0., 0.],
[0.,  0.,  167.,  0.,  0.,  71.,  0.,  0.,  171.,  0.,  0.,  25.,  0.,  0.,  183.,  0.,  0., 79., 0., 0., 187., 0., 0.],
[0.,  231.,  168.,  232.,  0.,  70.,  0.,  235.,  172.,  236.,  0.,  26.,  0.,  247.,  184.,  248.,  0., 78., 0., 251., 188., 252., 0.],
[0.,  0.,  0.,  0.,  0.,  69.,  0.,  0.,  0.,  0.,  0.,  27.,  0.,  0.,  0.,  0.,  0., 77., 0., 0., 0., 0., 0.],
[0.,  0.,  0.,  0.,  0.,  46.,  45.,  44.,  43.,  42.,  41.,  28.,  47.,  48.,49.,50.,51., 52., 0., 0., 0., 0., 0.],
[0.,  0.,  0.,  0.,  0.,  73.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 81., 0., 0., 0., 0., 0.],
[0.,  237.,  174.,  238.,  0.,  74.,  0.,  241.,  178.,  242.,  0.,  0.,  0.,  253.,  190.,  254.,  0., 82., 0., 257., 194., 258., 0.],
[0.,  0.,  173.,  0.,  0.,  75.,  0.,  0.,  177.,  0.,  0.,  0.,  0.,  0.,  189.,  0.,  0., 83., 0., 0., 193., 0., 0.],
[0.,  0.,  117.,  116., 115.,76.,  118., 119., 120.,  0.,  0.,  0.,  0.,  0., 129., 128., 127.,84., 130.,131.,132., 0., 0.],
[0.,  0.,  175.,  0.,  0.,  0.,  0.,  0.,  179.,  0.,  0.,  0.,  0.,  0.,  191.,  0.,  0., 0., 0., 0., 195., 0., 0.],
[0.,  239.,  176.,  240.,  0.,  0.,  0.,  243.,  180.,  244.,  0.,  0.,  0.,  255.,  192.,  256.,  0., 0., 0., 259., 196., 260., 0.],
[0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 0., 0., 0., 0., 0., 0.]])

plot_mazenode_with_labels(Maze, num_node)
env = Qmaze(maze=Maze)
model = QLearningAgent(env,qtable_path=False)
model.train(10)







